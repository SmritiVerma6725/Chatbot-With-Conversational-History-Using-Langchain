{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6ea578",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4eed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70f8f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c1700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Using cached langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
      "Collecting groq<1,>=0.4.1\n",
      "  Using cached groq-0.28.0-py3-none-any.whl (130 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.49\n",
      "  Using cached langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.14.0)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.4,>=0.3.45\n",
      "  Using cached langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Collecting packaging<25,>=23.2\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.3.0)\n",
      "Collecting certifi\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "     -------------------------------------- 157.7/157.7 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Using cached zstandard-0.23.0-cp310-cp310-win_amd64.whl (495 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.18-cp310-cp310-win_amd64.whl (134 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tenacity, sniffio, PyYAML, pydantic-core, packaging, orjson, jsonpointer, idna, h11, distro, charset_normalizer, certifi, annotated-types, requests, pydantic, jsonpatch, httpcore, anyio, requests-toolbelt, httpx, langsmith, groq, langchain-core, langchain-groq\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.6.15 charset_normalizer-3.4.2 distro-1.9.0 groq-0.28.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.65 langchain-groq-0.3.2 langsmith-0.3.45 orjson-3.10.18 packaging-24.2 pydantic-2.11.7 pydantic-core-2.33.2 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.1 urllib3-2.4.0 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c9c6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B0CEE06BC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B0CEE04670>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc26462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Smriti, it's a pleasure to meet you!\\n\\nBeing a Chief AI Engineer is a fascinating role. What kind of projects are you currently working on?  \\n\\nI'm always eager to learn more about the advancements happening in the field of AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 23, 'total_tokens': 80, 'completion_time': 0.103636364, 'prompt_time': 0.002113427, 'queue_time': 0.249860203, 'total_time': 0.105749791}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7325339-aee2-4a48-b94f-664530d6f430-0', usage_metadata={'input_tokens': 23, 'output_tokens': 57, 'total_tokens': 80})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Smriti and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099aa634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Smriti, and you are a Chief AI Engineer!  üòä \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm happy to chat. \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 99, 'total_tokens': 144, 'completion_time': 0.081818182, 'prompt_time': 0.004365863, 'queue_time': 0.25256314799999996, 'total_time': 0.086184045}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--15287748-7936-482a-b4f5-51de3f3c9b00-0', usage_metadata={'input_tokens': 99, 'output_tokens': 45, 'total_tokens': 144})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Smriti and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Smriti! It's nice to meet you. \\n\\nAs a Chief AI Engineer, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf5719",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebaca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain_community) (2.32.4)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain_community) (0.3.65)\n",
      "Collecting numpy>=1.26.2\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain_community) (0.3.45)\n",
      "Collecting langchain<1.0.0,>=0.3.25\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.12.13-cp310-cp310-win_amd64.whl (450 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.4.4-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.7)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (4.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.0)\n",
      "Installing collected packages: propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, greenlet, frozenlist, attrs, async-timeout, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-text-splitters, langchain, langchain_community\n",
      "Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-4.0.3 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.3 httpx-sse-0.4.0 langchain-0.3.25 langchain-text-splitters-0.3.8 langchain_community-0.3.25 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 numpy-2.2.6 propcache-0.3.2 pydantic-settings-2.9.1 typing-inspect-0.9.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfbe410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca9536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59dd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Smriti and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb95149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Smriti!  It's great to meet you. \\n\\nBeing a Chief AI Engineer is a fascinating role. What kind of AI projects are you leading these days?  I'm always interested in hearing about the innovative work being done in the field.  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42c48f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Smriti.  üòä \\n\\nI remembered from our earlier conversation. \\n\\n\\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 166, 'total_tokens': 198, 'completion_time': 0.058181818, 'prompt_time': 0.007017899, 'queue_time': 0.25173724000000003, 'total_time': 0.065199717}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2b38e866-0661-402a-9d41-37449582b52e-0', usage_metadata={'input_tokens': 166, 'output_tokens': 32, 'total_tokens': 198})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051f6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and don't know your name. If you'd like to tell me, I'm happy to know! üòä\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1955ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, John! üëã  Is there anything I can help you with today? \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084c221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John,  remember? üòä  How can I help you today?\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e761899",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f5abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec2d12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Smriti, it's nice to meet you!\\n\\nI'm happy to help with any questions you have. Just ask away!  üòä \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 32, 'total_tokens': 67, 'completion_time': 0.063636364, 'prompt_time': 0.002342206, 'queue_time': 0.253632804, 'total_time': 0.06597857}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--7e058363-1f2c-4082-8bec-e5d9645f2085-0', usage_metadata={'input_tokens': 32, 'output_tokens': 35, 'total_tokens': 67})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Smriti\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55be74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d707cdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Smriti, it's nice to meet you! \\n\\nI'm happy to help with any questions you have. Just ask away, and I'll do my best to provide you with useful and informative answers. üòä \\n\\nWhat can I do for you today?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 32, 'total_tokens': 94, 'completion_time': 0.112727273, 'prompt_time': 0.002358676, 'queue_time': 0.25102839, 'total_time': 0.115085949}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--eab43e13-0c88-4bec-bd84-999dd6e69d9a-0', usage_metadata={'input_tokens': 32, 'output_tokens': 62, 'total_tokens': 94})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Smriti\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf723ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Smriti.  I remember! üòä  \\n\\nHow can I help you further? \\n\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9cbd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b445bbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§∏‡•ç‡§Æ‡•É‡§§‡§ø! üëã \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ üòä ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§  \\n\\n‡§Ü‡§™ ‡§ï‡§ø‡§∏ ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•Ä ‡§π‡•à‡§Ç?  \\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Smriti\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36533fe",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad1128e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5800cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á Smriti! ‡§Æ‡•Å‡§ù‡•á ‡§¨‡§§‡§æ‡§ì, ‡§Æ‡•à‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å? üòä \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Smriti\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d50bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ade965f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§®‡§æ‡§Æ Smriti ‡§π‡•à! üòä \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278ec51",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "152f58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\onedrive\\desktop\\chatbotgenai\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 huggingface-hub-0.33.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0697e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\OneDrive\\Desktop\\ChatbotGenAI\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\KIIT0001\\OneDrive\\Desktop\\ChatbotGenAI\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KIIT0001\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52aa6765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to your personal information, including your favorite ice cream flavor! \\n\\nWhat's your favorite ice cream? üòäüç¶\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b37088f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked: 2 + 2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0e0cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac77114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any personal information about you, including your name.  If you'd like to tell me your name, I'd be happy to know! üòä  \\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dab2b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful assistant, I have no memory of past conversations. If you'd like to ask me a math problem, I'm happy to help! üòä  Just ask away! \\n\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13087c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
